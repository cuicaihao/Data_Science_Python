{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your own Deep Learning framework using Numpy\n",
    "\n",
    "https://quantdare.com/create-your-own-deep-learning-framework-using-numpy/\n",
    "\n",
    "1. Neural networks in a nutshell\n",
    "2. Gradient descent and backpropagation\n",
    "3. Putting things together\n",
    "4. Give me the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is! A neural network will be created using the Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = []\n",
    "     \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "     \n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        for i, _ in enumerate(self.layers):\n",
    "            forward = self.layers[i].forward(X)\n",
    "            X = forward\n",
    "             \n",
    "        return forward\n",
    "     \n",
    "    def train(\n",
    "        self, \n",
    "        X_train, \n",
    "        Y_train, \n",
    "        learning_rate, \n",
    "        epochs, \n",
    "        verbose=False\n",
    "    ):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self._run_epoch(X_train, Y_train, learning_rate)\n",
    "             \n",
    "            if verbose:\n",
    "                if epoch % 50 == 0:\n",
    "                    print(f'Epoch: {epoch}. Loss: {loss}')\n",
    "     \n",
    "    def _run_epoch(self, X, Y, learning_rate):\n",
    "        # Forward pass\n",
    "        for i, _ in enumerate(self.layers):\n",
    "            forward = self.layers[i].forward(input_val=X)\n",
    "            X = forward\n",
    "             \n",
    "        # Compute loss and first gradient\n",
    "        bce = BinaryCrossEntropy(forward, Y)\n",
    "        error = bce.forward()\n",
    "        gradient = bce.backward()\n",
    "         \n",
    "        self.loss.append(error)\n",
    "         \n",
    "        # Backpropagation\n",
    "        for i, _ in reversed(list(enumerate(self.layers))):\n",
    "            if self.layers[i].type != 'Linear':\n",
    "                gradient = self.layers[i].backward(gradient)\n",
    "            else:\n",
    "                gradient, dW, dB = self.layers[i].backward(gradient)\n",
    "                self.layers[i].optimize(dW, dB, learning_rate)\n",
    "                 \n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the class Model has 3 methods: add, train and predict that allow us to control the network behaviour.\n",
    "\n",
    "The private method _run_epoch computes only one epoch. It does it by following the next procedure:\n",
    "\n",
    "- Compute forward pass.\n",
    "- Calculate error and gradient on the last layer.\n",
    "- Backpropagates the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we don’t actually need the error in backpropagation, just the gradient. We use the error to see how far we are from our objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Layer abstract class\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "     \n",
    "    def __len__(self):\n",
    "        pass\n",
    "     \n",
    "    def __str__(self):\n",
    "        pass\n",
    "     \n",
    "    def forward(self):\n",
    "        pass\n",
    "     \n",
    "    def backward(self):\n",
    "        pass\n",
    "     \n",
    "    def optimize(self):\n",
    "        pass\n",
    " \n",
    " \n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.rand(output_dim, input_dim)\n",
    "        self.biases = np.random.rand(output_dim, 1)\n",
    "        self.type = 'Linear'\n",
    " \n",
    "    def __str__(self):\n",
    "        return f\"{self.type} Layer\"\n",
    "         \n",
    "    def forward(self, input_val):\n",
    "        self._prev_acti = input_val\n",
    "        return np.matmul(self.weights, input_val) + self.biases\n",
    "     \n",
    "    def backward(self, dA):\n",
    "        dW = np.dot(dA, self._prev_acti.T)\n",
    "        dB = dA.mean(axis=1, keepdims=True)\n",
    "         \n",
    "        delta = np.dot(self.weights.T, dA)\n",
    "         \n",
    "        return delta, dW, dB\n",
    "     \n",
    "    def optimize(self, dW, dB, rate):\n",
    "        self.weights = self.weights - rate * dW\n",
    "        self.biases = self.biases - rate * dB\n",
    " \n",
    " \n",
    "class ReLU(Layer):    \n",
    "    def __init__(self, output_dim):\n",
    "        self.units = output_dim\n",
    "        self.type = 'ReLU'\n",
    " \n",
    "    def __str__(self):\n",
    "        return f\"{self.type} Layer\"       \n",
    "         \n",
    "    def forward(self, input_val):\n",
    "        self._prev_acti = np.maximum(0, input_val)\n",
    "        return self._prev_acti\n",
    "     \n",
    "    def backward(self, dJ):\n",
    "        return dJ * np.heaviside(self._prev_acti, 0)\n",
    " \n",
    " \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        self.units = output_dim\n",
    "        self.type = 'Sigmoid'\n",
    " \n",
    "    def __str__(self):\n",
    "        return f\"{self.type} Layer\"       \n",
    "         \n",
    "    def forward(self, input_val):\n",
    "        self._prev_acti = 1 / (1 + np.exp(-input_val))\n",
    "        return self._prev_acti\n",
    "     \n",
    "    def backward(self, dJ):\n",
    "        sig = self._prev_acti\n",
    "        return dJ * sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the error, we have a lot of options. Probably, the most basic one is the Mean Squared Error we saw earlier. I have added another one called Binary Cross-Entropy (the one that is in the code) because we will test our model using the latter in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Layer):\n",
    "    def __init__(self, predicted, real):\n",
    "        self.predicted = predicted\n",
    "        self.real = real\n",
    "        self.type = 'Mean Squared Error'\n",
    "     \n",
    "    def forward(self):\n",
    "        return np.power(self.predicted - self.real, 2).mean()\n",
    " \n",
    "    def backward(self):\n",
    "        return 2 * (self.predicted - self.real).mean()\n",
    " \n",
    " \n",
    "class BinaryCrossEntropy(Layer):\n",
    "    def __init__(self, predicted, real):\n",
    "        self.real = real\n",
    "        self.predicted = predicted\n",
    "        self.type = 'Binary Cross-Entropy'\n",
    "     \n",
    "    def forward(self):\n",
    "        n = len(self.real)\n",
    "        loss = np.nansum(-self.real * np.log(self.predicted) - (1 - self.real) * np.log(1 - self.predicted)) / n\n",
    "         \n",
    "        return np.squeeze(loss)\n",
    "     \n",
    "    def backward(self):\n",
    "        n = len(self.real)\n",
    "        return (-(self.real / self.predicted) + ((1 - self.real) / (1 - self.predicted))) / n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers can compute in 2 directions: forward and backward. This is an inherited behaviour from the computational graphs design, and it makes computationally easier to calculate the derivatives. In fact, we could have split the Linear layer into “multiply and “add” classes, as TensorFlow does it.\n",
    "\n",
    "The weights and biases are initialized using a uniform distribution. There are other ways to initialize these parameters, like kaiming initialization.\n",
    "\n",
    "The forward pass of a linear layer just computes the formula of a neuron we saw previously. The backward pass is a little trickier to understand: once we compute the gradient on the last layer, we backpropagate it by multiplying the corresponding derivatives of the actual layer with the incoming gradient of the following layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(samples, shape_type='circles', noise=0.05):\n",
    "    # We import in the method for the shake of simplicity\n",
    "\n",
    "    if shape_type is 'moons':\n",
    "        X, Y = make_moons(n_samples=samples, noise=noise)\n",
    "    elif shape_type is 'circles':\n",
    "        X, Y = make_circles(n_samples=samples, noise=noise)\n",
    "    else:\n",
    "        raise ValueError(f\"The introduced shape {shape_type} is not valid. Please use 'moons' or 'circles' \")\n",
    "     \n",
    "    data = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))\n",
    "     \n",
    "    return data\n",
    " \n",
    "def plot_generated_data(data):\n",
    "    ax = data.plot.scatter(x='x', y='y', figsize=(16,12), color=data['label'], \n",
    "                 cmap=matplotlib.colors.ListedColormap(['skyblue', 'salmon']), grid=True);\n",
    "     \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles \n",
    "data = generate_data(samples=5000, shape_type='moons', noise=0.04)\n",
    "plot_generated_data(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation and addition of layers to the model is very straightforward because it works pretty much the same as in Keras. Below you will find the code to create and train a classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = data[['x', 'y']].values\n",
    "Y = data['label'].T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Model()\n",
    "model.add(Linear(2, 5))\n",
    "model.add(ReLU(5))\n",
    " \n",
    "model.add(Linear(5,5))\n",
    "model.add(ReLU(5))\n",
    " \n",
    "model.add(Linear(5,1))\n",
    "model.add(Sigmoid(1))\n",
    "\n",
    "# Train model\n",
    "model.train(X_train = X.T, \n",
    "            Y_train = Y, \n",
    "            learning_rate = 0.05, \n",
    "            epochs=9000,\n",
    "            verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can plot the loss of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,10))\n",
    "plt.plot(model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss curve is not ideal, but is good enough for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model, steps=1000, cmap='Paired'):\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "\n",
    "    # Define region of interest by data limits\n",
    "    xmin, xmax = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    ymin, ymax = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "    steps = 1000\n",
    "    x_span = np.linspace(xmin, xmax, steps)\n",
    "    y_span = np.linspace(ymin, ymax, steps)\n",
    "    xx, yy = np.meshgrid(x_span, y_span)\n",
    "\n",
    "    # Make predictions across region of interest\n",
    "#     labels = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    XN = np.c_[xx.ravel(), yy.ravel()]\n",
    "    labels = model.predict(XN.T).T\n",
    "\n",
    "    # Plot decision boundary in region of interest\n",
    "    z = labels.reshape(xx.shape)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, z, cmap=cmap, alpha=0.5)\n",
    "\n",
    "    # Get predicted labels on training data and plot\n",
    "    train_labels = model.predict(X.T).T\n",
    "    ax.scatter(X[:,0], X[:,1], c=train_labels, cmap=cmap, lw=0)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X, Y, model, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    " \n",
    "# Make predictions\n",
    "predictions = model.predict(X.T).T\n",
    " \n",
    "# Format the predictions\n",
    "new_pred = []\n",
    " \n",
    "for p in predictions:\n",
    "    if p < 0.5:\n",
    "        new_pred.append(0)\n",
    "    else:\n",
    "        new_pred.append(1)\n",
    "        \n",
    "# Calculate the score\n",
    "roc_auc_score(y_true=Y, y_score=new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model object\n",
    "model = Sequential()\n",
    "\n",
    "# kwarg dict for convenience\n",
    "layer_kw = dict(activation='sigmoid')\n",
    "\n",
    "# Add layers to our model\n",
    "model.add(Dense(units=5, input_shape=(2, ), **layer_kw))\n",
    "model.add(Dense(units=5, **layer_kw))\n",
    "model.add(Dense(units=1, **layer_kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=sgd, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X , Y , verbose=0,  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model, steps=1000, cmap='Paired'):\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "\n",
    "    # Define region of interest by data limits\n",
    "    xmin, xmax = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    ymin, ymax = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "    steps = 1000\n",
    "    x_span = np.linspace(xmin, xmax, steps)\n",
    "    y_span = np.linspace(ymin, ymax, steps)\n",
    "    xx, yy = np.meshgrid(x_span, y_span)\n",
    "\n",
    "    # Make predictions across region of interest\n",
    "    labels = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Plot decision boundary in region of interest\n",
    "    z = labels.reshape(xx.shape)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, z, cmap=cmap, alpha=0.5)\n",
    "\n",
    "    # Get predicted labels on training data and plot\n",
    "    train_labels = model.predict(X)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y, cmap=cmap, lw=0)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "plot_decision_boundary(X, Y, model, cmap='RdBu')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c8cc877dfaaf169ac8b8c232703907b2186a0f11d6917571021133971a8d95"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('py37automl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
