{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "========================================\n",
    "\n",
    "# Release Highlights for scikit-learn 0.23\n",
    "\n",
    "========================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "We are pleased to announce the release of scikit-learn 0.23! Many bug fixes\n",
    "and improvements were added, as well as some new key features. We detail\n",
    "below a few of the major features of this release. **For an exhaustive list of\n",
    "all the changes**, please refer to the `release notes <changes_0_23>`.\n",
    "\n",
    "To install the latest version (with pip)::\n",
    "\n",
    "    pip install --upgrade scikit-learn\n",
    "\n",
    "or with conda::\n",
    "\n",
    "    conda install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalized Linear Models, and Poisson loss for gradient boosting\n",
    "-----------------------------------------------------------------\n",
    "Long-awaited Generalized Linear Models with non-normal loss functions are now\n",
    "available. In particular, three new regressors were implemented:\n",
    ":class:`~sklearn.linear_model.PoissonRegressor`,\n",
    ":class:`~sklearn.linear_model.GammaRegressor`, and\n",
    ":class:`~sklearn.linear_model.TweedieRegressor`. The Poisson regressor can be\n",
    "used to model positive integer counts, or relative frequencies. Read more in\n",
    "the `User Guide <Generalized_linear_regression>`. Additionally,\n",
    ":class:`~sklearn.ensemble.HistGradientBoostingRegressor` supports a new\n",
    "'poisson' loss as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "n_samples, n_features = 1000, 20\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "# positive integer target correlated with X[:, 5] with many zeros:\n",
    "y = rng.poisson(lam=np.exp(X[:, 5]) / 2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n",
    "glm = PoissonRegressor()\n",
    "gbdt = HistGradientBoostingRegressor(loss='poisson', learning_rate=.01)\n",
    "glm.fit(X_train, y_train)\n",
    "gbdt.fit(X_train, y_train)\n",
    "print(glm.score(X_test, y_test))\n",
    "print(gbdt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rich visual representation of estimators\n",
    "-----------------------------------------\n",
    "Estimators can now be visualized in notebooks by enabling the\n",
    "`display='diagram'` option. This is particularly useful to summarise the\n",
    "structure of pipelines and other composite estimators, with interactivity to\n",
    "provide detail.  Click on the example image below to expand Pipeline\n",
    "elements.  See `visualizing_composite_estimators` for how you can use\n",
    "this feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "set_config(display='diagram')\n",
    "\n",
    "num_proc = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n",
    "\n",
    "cat_proc = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='missing'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((num_proc, ('feat1', 'feat3')),\n",
    "                                       (cat_proc, ('feat0', 'feat2')))\n",
    "\n",
    "clf = make_pipeline(preprocessor, LogisticRegression())\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalability and stability improvements to KMeans\n",
    "------------------------------------------------\n",
    "The :class:`~sklearn.cluster.KMeans` estimator was entirely re-worked, and it\n",
    "is now significantly faster and more stable. In addition, the Elkan algorithm\n",
    "is now compatible with sparse matrices. The estimator uses OpenMP based\n",
    "parallelism instead of relying on joblib, so the `n_jobs` parameter has no\n",
    "effect anymore. For more details on how to control the number of threads,\n",
    "please refer to our `parallelism` notes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import completeness_score\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X, y = make_blobs(random_state=rng)\n",
    "X = scipy.sparse.csr_matrix(X)\n",
    "X_train, X_test, _, y_test = train_test_split(X, y, random_state=rng)\n",
    "kmeans = KMeans(algorithm='elkan').fit(X_train)\n",
    "print(completeness_score(kmeans.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements to the histogram-based Gradient Boosting estimators\n",
    "----------------------------------------------------------------\n",
    "Various improvements were made to\n",
    ":class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n",
    ":class:`~sklearn.ensemble.HistGradientBoostingRegressor`. On top of the\n",
    "Poisson loss mentionned above, these estimators now support `sample\n",
    "weights <sw_hgbdt>`. Also, an automatic early-stopping criterion was added:\n",
    "early-stopping is enabled by default when the number of samples exceeds 10k.\n",
    "Finally, users can now define `monotonic constraints\n",
    "<monotonic_cst_gbdt>` to constrain the predictions based on the variations of\n",
    "specific features. In the following example, we construct a target that is\n",
    "generally positively correlated with the first feature, with some noise.\n",
    "Applying monotoinc constraints allows the prediction to capture the global\n",
    "effect of the first feature, instead of fitting the noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "n_samples = 500\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(n_samples, 2)\n",
    "noise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n",
    "y = (5 * X[:, 0] + np.sin(10 * np.pi * X[:, 0]) - noise)\n",
    "\n",
    "gbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)\n",
    "gbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)\n",
    "\n",
    "disp = plot_partial_dependence(\n",
    "    gbdt_no_cst, X, features=[0], feature_names=['feature 0'],\n",
    "    line_kw={'linewidth': 4, 'label': 'unconstrained'})\n",
    "plot_partial_dependence(gbdt_cst, X, features=[0],\n",
    "    line_kw={'linewidth': 4, 'label': 'constrained'}, ax=disp.axes_)\n",
    "disp.axes_[0, 0].plot(X[:, 0], y, 'o', alpha=.5, zorder=-1, label='samples')\n",
    "disp.axes_[0, 0].set_ylim(-3, 3); disp.axes_[0, 0].set_xlim(-1, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample-weight support for Lasso and ElasticNet\n",
    "----------------------------------------------\n",
    "The two linear regressors :class:`~sklearn.linear_model.Lasso` and\n",
    ":class:`~sklearn.linear_model.ElasticNet` now support sample weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "n_samples, n_features = 1000, 20\n",
    "rng = np.random.RandomState(0)\n",
    "X, y = make_regression(n_samples, n_features, random_state=rng)\n",
    "sample_weight = rng.rand(n_samples)\n",
    "X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n",
    "    X, y, sample_weight, random_state=rng)\n",
    "reg = Lasso()\n",
    "reg.fit(X_train, y_train, sample_weight=sw_train)\n",
    "print(reg.score(X_test, y_test, sw_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c8cc877dfaaf169ac8b8c232703907b2186a0f11d6917571021133971a8d95"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('py37automl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
