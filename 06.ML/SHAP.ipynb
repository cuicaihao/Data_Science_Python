{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to explainable AI with Shapley values\n",
    "\n",
    "This is an introduction to explaining machine learning models with Shapley values. Shapley values are a widely used approach from cooperative game theory that come with desirable properties. This tutorial is designed to help build a solid understanding of how to compute and interpet Shapley-based explanations of machine learning models. We will take a practical hands-on approach, using the shap Python package to explain progressively more complex models. This is a living document, and serves as an introduction to the shap Python package. So if you have feedback or contributions please open an issue or pull request to make this tutorial better!\n",
    "\n",
    "Outline\n",
    "\n",
    "- Explaining a linear regression model\n",
    "- Explaining a generalized additive regression model\n",
    "- Explaining a non-additive boosted tree model\n",
    "- Explaining a linear logistic regression model\n",
    "- Explaining a non-additive boosted tree logistic regression model\n",
    "- Dealing with correlated input features\n",
    "- Explaining a transformers NLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining a linear regression model\n",
    "\n",
    "Before using Shapley values to explain complicated models, it is helpful to understand how they work for simple models. One of the simplest model types is standard linear regression, and so below we train a linear regression model on the [California housing dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). This dataset consists of 20,640 blocks of houses across California in 1990, where our goal is to predict the natural log of the median home price from 8 different features:\n",
    "\n",
    "1. MedInc - median income in block group\n",
    "1. HouseAge - median house age in block group\n",
    "1. AveRooms - average number of rooms per household\n",
    "1. AveBedrms - average number of bedrooms per household\n",
    "1. Population - block group population\n",
    "1. AveOccup - average number of household members\n",
    "1. Latitude - block group latitude\n",
    "1. Longitude - block group longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "import shap\n",
    "\n",
    "# a classic housing price dataset\n",
    "X, y = shap.datasets.california(n_points=1000)\n",
    "\n",
    "X100 = shap.utils.sample(X, 100)  # 100 instances for use as the background distribution\n",
    "\n",
    "# a simple linear model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the model coefficients\n",
    "\n",
    "The most common way of understanding a linear model is to examine the coefficients learned for each feature. These coefficients tell us how much the model output changes when we change each of the input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.head(), type(X), type(y), y\n",
    "X.shape, y.shape, type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model coefficients:\\n\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(X.columns[i], \"=\", model.coef_[i].round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While coefficients are great for telling us what will happen when we change the value of an input feature, by themselves they are not a great way to measure the overall importance of a feature. \n",
    "\n",
    "This is because the value of each coefficient depends on the scale of the input features. If for example we were to measure the age of a home in minutes instead of years, then the coefficients for the HouseAge feature would become 0.0115 / (365∗24∗60) = 2.18e-8. \n",
    "\n",
    "Clearly the number of years since a house was built is not more important than the number of minutes, yet its coefficient value is much larger. This means that the magnitude of a coefficient is not necessarily a good measure of a feature’s importance in a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complete picture using partial dependence plots\n",
    "\n",
    "To understand a feature’s importance in a model, it is necessary to understand both how changing that feature impacts the model’s output, and also the distribution of that feature’s values. To visualize this for a linear model we can build a classical partial dependence plot and show the distribution of feature values as a histogram on the x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions for the 100 instances\n",
    "y100 = model.predict(X100)\n",
    "\n",
    "# Get the mean value of the predictions\n",
    "y100.mean(), y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X100['MedInc'].mean(),  X100['MedInc'].std(), X100['MedInc'].min(), X100['MedInc'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model.predict,\n",
    "    X100,\n",
    "    ice=True,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gray horizontal line in the plot above represents the expected value of the model when applied to the California housing dataset. \n",
    "\n",
    "The vertical gray line represents the average value of the median income feature. \n",
    "\n",
    "Note that the blue partial dependence plot line (which is the average value of the model output when we fix the median income feature to a given value) always passes through the intersection of the two gray expected value lines. \n",
    "\n",
    "We can consider this intersection point as the “center” of the partial dependence plot with respect to the data distribution. The impact of this centering will become clear when we turn to Shapley values next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading SHAP values from partial dependence plots\n",
    "\n",
    "The core idea behind Shapley value based explanations of machine learning models is to use fair allocation results from cooperative game theory to allocate credit for a model’s output $f(x)$ among its input features.\n",
    "\n",
    "In order to connect game theory with machine learning models, it is necessary to both match a model’s input features with players in a game, and also match the model function with the rules of the game. \n",
    "\n",
    "Since in game theory a player can join or not join a game, we need a way for a feature to “join” or “not join” a model. The most common way to define what it means for a feature to “join” a model is to say that feature has “joined a model” when we know the value of that feature, and it has not joined a model when we don’t know the value of that feature.\n",
    "\n",
    "To evaluate an existing model $f$ when only a subset $S$ of features are part of the model we integrate out the other features using a conditional expected value formulation. This formulation can take two forms:\n",
    "\n",
    "$$ E\\left[f(X) | X_S = x_S\\right] $$\n",
    "\n",
    "or \n",
    "\n",
    "$$ E\\left[f(X) | do (X_S = x_S)\\right] $$\n",
    "\n",
    "In the first form we know the values of the features in $S$ because we observe them. In the second form we know the values of the features in $S$ because we set them. \n",
    "\n",
    "In general, the second form is usually preferable, both because it tells us how the model would behave if we were to intervene and change its inputs, and also because it is much easier to compute. \n",
    "\n",
    "In this tutorial we will focus entirely on the second formulation. We will also use the more specific term “SHAP values” to refer to Shapley values applied to a conditional expectation function of a machine learning model.\n",
    "\n",
    "SHAP values can be very complicated to compute (they are NP-hard in general), but linear models are so simple that we can read the SHAP values right off a partial dependence plot. \n",
    "\n",
    "When we are explaining a prediction $f(x)$, the SHAP value for a specific feature $i$ is just the difference between the expected model output and the partial dependence plot at the feature’s value $x_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the SHAP values for the linear model\n",
    "explainer = shap.Explainer(model.predict, X100)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# make a standard partial dependence plot\n",
    "sample_ind = 20\n",
    "shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model.predict,\n",
    "    X100,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    ice=False,\n",
    "    shap_values=shap_values[sample_ind : sample_ind + 1, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The close correspondence between the classic partial dependence plot and SHAP values means that if we plot the SHAP value for a specific feature across a whole dataset we will exactly trace out a mean centered version of the partial dependence plot for that feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:, \"MedInc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The additive nature of Shapley values\n",
    "\n",
    "One of the fundamental properties of Shapley values is that they always sum up to the difference between the game outcome when all players are present and the game outcome when no players are present. \n",
    "\n",
    "For machine learning models this means that SHAP values of all the input features will always sum up to the difference between baseline (expected) model output and the current model output for the prediction being explained. \n",
    "\n",
    "The easiest way to see this is through a waterfall plot that starts at our background prior expectation for a home price $E(f(X))$, and then adds features one at a time until we reach the current model output $f(x)$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the waterfall_plot shows how we get from shap_values.base_values to model.predict(X)[sample_ind]\n",
    "shap.plots.waterfall(shap_values[sample_ind], max_display=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining an additive regression model\n",
    "\n",
    "s have such a close connection to SHAP values is because each feature in the model is handled independently of every other feature (the effects are just added together). We can keep this additive nature while relaxing the linear requirement of straight lines. \n",
    "\n",
    "This results in the well-known class of generalized additive models (GAMs). While there are many ways to train these types of models (like setting an XGBoost model to depth-1), we will use InterpretMLs explainable boosting machines that are specifically designed for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a GAM model to the data\n",
    "import interpret.glassbox\n",
    "\n",
    "model_ebm = interpret.glassbox.ExplainableBoostingRegressor(interactions=0)\n",
    "model_ebm.fit(X, y)\n",
    "\n",
    "# explain the GAM model with SHAP\n",
    "explainer_ebm = shap.Explainer(model_ebm.predict, X100)\n",
    "shap_values_ebm = explainer_ebm(X)\n",
    "\n",
    "# make a standard partial dependence plot with a single SHAP value overlaid\n",
    "fig, ax = shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model_ebm.predict,\n",
    "    X100,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    show=False,\n",
    "    ice=False,\n",
    "    shap_values=shap_values_ebm[sample_ind : sample_ind + 1, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_ebm[:, \"MedInc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the waterfall_plot shows how we get from explainer.expected_value to model.predict(X)[sample_ind]\n",
    "shap.plots.waterfall(shap_values_ebm[sample_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the waterfall_plot shows how we get from explainer.expected_value to model.predict(X)[sample_ind]\n",
    "shap.plots.beeswarm(shap_values_ebm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost model\n",
    "import xgboost\n",
    "\n",
    "model_xgb = xgboost.XGBRegressor(n_estimators=100, max_depth=2).fit(X, y)\n",
    "\n",
    "# explain the GAM model with SHAP\n",
    "explainer_xgb = shap.Explainer(model_xgb, X100)\n",
    "shap_values_xgb = explainer_xgb(X)\n",
    "\n",
    "# make a standard partial dependence plot with a single SHAP value overlaid\n",
    "fig, ax = shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    model_xgb.predict,\n",
    "    X100,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    "    show=False,\n",
    "    ice=False,\n",
    "    shap_values=shap_values_xgb[sample_ind : sample_ind + 1, :],\n",
    ")\n",
    "\n",
    "\n",
    "shap.plots.scatter(shap_values_xgb[:, \"MedInc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:, \"MedInc\"], color=shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
