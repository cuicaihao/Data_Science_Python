{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Exploring Word Embeddings\n",
    "In this notebook, we'll look at trained word embeddings. We'll plot the embeddings so we can attempt to visually compare embeddings. We'll then look at analogies and word similarities. We'll use the Gensim library which makes it easy to work with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download a table of pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embeddings (66MB, glove, trained on wikipedia)\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the embedding of 'king'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many words does this table have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means:\n",
    "* 400,000 words (vocab_size)\n",
    "* Each has an embedding composed of 50 numbers (embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the embedding vector\n",
    "Let's plot the vector so we can have a colorful visual of values in the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_embeddings(vectors, labels=None):\n",
    "    n_vectors = len(vectors)\n",
    "    fig = plt.figure(figsize=(12, n_vectors))\n",
    "    # ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    # ax = fig.add_axes([1, 1, 1, 1])\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    sns.heatmap(vectors, cmap='RdBu', vmax=2, vmin=-2, ax=ax)\n",
    "    \n",
    "    if labels:\n",
    "        ax.set_yticklabels(labels,rotation=0)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        \n",
    "    plt.tick_params(axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False) # labels along the bottom edge are off\n",
    "    \n",
    "    # From https://github.com/mwaskom/seaborn/issues/1773\n",
    "    # fix for mpl bug that cuts off top/bottom of seaborn viz\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    plt.show() # ta-da!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the embedding of `king`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings([model['king']], ['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare multiple embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings([model['king'], model['man'], model['woman'], model['girl'], model['boy']],\n",
    "              ['king', 'man', 'woman', 'girl', 'boy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example including a number of different concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings([model['king'], model['water'], model['god'], model['love'], model['star']],\n",
    "              ['king', 'water', 'god', 'love', 'star'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies\n",
    "### king - man + woman  = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings([model['king'], \n",
    "                model['man'], \n",
    "                model['woman'],\n",
    "                model['king'] - model['man'] + model['woman'],\n",
    "                model['queen']],\n",
    "                ['king', 'man', 'woman', 'king-man+woman', 'queen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2019 update**: This turned out to be a misconception. The result is actually closer to \"king\" than it is to \"queen\", it's just that the code rules out the input vectors as possible outputs\n",
    "\n",
    "\n",
    "[Fair is Better than Sensational:Man is to Doctor as Woman is to Doctor](https://arxiv.org/abs/1905.09866)\n",
    "\n",
    "To verify, let's calculate cosine distance between the result of the analogy, and `queen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model['king'] - model['man'] + model['woman']\n",
    "\n",
    "# Similarity between result and 'queen'\n",
    "cosine_similarity(result.reshape(1, -1), model['queen'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare that to the distance between the result and `king`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity between result and 'king'\n",
    "cosine_similarity(result.reshape(1, -1), model['king'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the result is more similar to king (0.8859834 similarity score) than it is to queen (0.8609581 similarity score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings( [model['king'],\n",
    "                 result, \n",
    "                 model['queen']],\n",
    "                 ['king', 'king-man+woman', 'queen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: doctor - man + woman = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill-in values\n",
    "model.most_similar(positive=[\"doctor\", \"man\"], negative=[\"woman\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify: Is it, really?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do analogy algebra\n",
    "result = model['doctor'] - model['man'] + model['woman']\n",
    "\n",
    "# Similarity between result and 'nurse'\n",
    "cosine_similarity(result.reshape(1, -1), model['nurse'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Similarity between result and 'doctor'\n",
    "cosine_similarity(result.reshape(1, -1), model['doctor'].reshape(1, -1))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01 - Trained Word Embeddings with Gensim.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python37664bitnlpcondac288fe709c1f4a20ba3c1c2e82bc8cad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
