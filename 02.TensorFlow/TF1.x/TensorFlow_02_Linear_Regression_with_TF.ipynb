{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow operations (also called **ops** for short) can take any number of inputs and produce any number of outputs. For example, the addition and multiplication ops each take two inputs and produce one output. \n",
    "\n",
    "For example, the addition and multiplication ops each take two inputs and produce one output. Constants and variables take no input (they are called **source ops**). \n",
    "\n",
    "The inputs and outputs are **multidimensional arrays**, called `tensors` (hence the name **“tensor flow”**).\n",
    "\n",
    "Just like NumPy arrays, tensors have a type and a shape. In fact, in the Python API tensors are simply represented by NumPy ndarrays. They typically contain floats, but you can also use them to carry strings (arbitrary byte arrays)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the following code manipulates 2D arrays to perform Linear Regression on the California housing data‐ set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "\n",
    "It starts by fetching the dataset; then it adds an extra bias input feature (`x0 = 1`) to all training instances (it does so using NumPy so it runs immediately); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "m,n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it creates two TensorFlow constant nodes, `X` and `y`, to hold this data and the targets, and it uses some of the matrix operations provided by TensorFlow to define theta. \n",
    "\n",
    "These matrix functions—`transpose()`, `matmul()`, and `matrix_inverse()`— are self-explanatory, but as usual they do not perform any computations immediately; instead, they create nodes in the graph that will perform them when the graph is run. \n",
    "\n",
    "You may recognize that the definition of `theta` corresponds to the Normal Equation\n",
    "$$\\theta = (X^{T} X)^{-1}X^{T}y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(housing_data_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = 'y')\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT), y)  \n",
    "y_pred = tf.matmul(X, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that housing.target is a 1D array, but we need to reshape it to a column vector to compute theta. \n",
    "Recall that NumPy’s reshape() function accepts –1 (meaning “unspecified”) for one of the dimensions: that dimension will be computed based on the array’s length and the remaining dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing.target.shape)\n",
    "print(housing.target.reshape(-1,1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the code creates a session and uses it to evaluate theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    y_pred_value = y_pred.eval()\n",
    "    \n",
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# By running this special iPython command, we will be displaying plots inline:\n",
    "%matplotlib inline \n",
    "\n",
    "id_sample = range(100);\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(housing.target[id_sample], 'b', label = 'housing.target')\n",
    "plt.plot(y_pred_value[id_sample], 'r', label = 'y_pred_values')\n",
    "plt.ylabel('House price')\n",
    "plt.xlabel('Samples')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main benefit of this code versus computing the Normal Equation directly using NumPy is that TensorFlow will automatically run this on your GPU card if you have one.\n",
    "\n",
    "We could also use other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try using `Batch Gradient Descent` instead of the `Normal Equation`. \n",
    "First we will do this by manually computing the gradients, then we will use `TensorFlow’s autodiff` feature to let TensorFlow compute the gradients automatically, and finally we will use a couple of TensorFlow’s out-of-the-box optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: When using Gradient Descent, remember that it is important to first normalize the input feature vectors, or else training may be much slower. You can do this using TensorFlow, NumPy, Scikit-Learn’s StandardScaler, or any other solution you prefer. The following code assumes that this normalization has already been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Computing the Gradients\n",
    "\n",
    "The following code should be fairly self-explanatory, except for a few new elements:\n",
    "- The `random_uniform()` function creates a node in the graph that will generate a tensor containing random values, given its shape and value range, much like NumPy’s rand() function.\n",
    "- The `assign()` function creates a node that will assign a new value to a variable. In this case, it implements the Batch Gradient Descent step $\\theta^{next step} = \\theta –  \\bigtriangledown_{\\theta}MSE(\\theta)$.\n",
    "- The main loop executes the training step over and over again (n_epochs times), and every 100 iterations it prints out the current Mean Squared Error (mse). \n",
    "You should see the MSE go down at every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = 'y')\n",
    "theta  =  tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = 'theta')\n",
    "y_pred = tf.matmul(X, theta, name = 'predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = 'mse')\n",
    "\n",
    "gradients = 2.0/m*tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate*gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)        \n",
    "    y_pred_value = y_pred.eval()    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"best_theta:\")    \n",
    "print(best_theta)\n",
    "\n",
    "# something went wrong here, I need to check why the mse is 'non'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_sample = range(100);\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(housing.target[id_sample], 'b', label = 'housing.target')\n",
    "plt.plot(y_pred_value[id_sample], 'r', label = 'y_pred_values')\n",
    "plt.ylabel('House price')\n",
    "plt.xlabel('Samples')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using autodiff\n",
    "\n",
    "The preceding code works fine, but it requires mathematically deriving the gradients from the cost function (MSE). In the case of Linear Regression, it is reasonably easy, but if you had to do this with deep neural networks you would get quite a headache: it would be tedious and error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = 'y')\n",
    "theta  =  tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = 'theta')\n",
    "y_pred = tf.matmul(X, theta, name = 'predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = 'mse')\n",
    "\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 200 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    y_pred_value = y_pred.eval()\n",
    "    best_theta = theta.eval()\n",
    "print(\"best_theta:\")     \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_sample = range(100);\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(housing.target[id_sample], 'b', label = 'housing.target')\n",
    "plt.plot(y_pred_value[id_sample], 'r', label = 'y_pred_values')\n",
    "plt.ylabel('House price')\n",
    "plt.xlabel('Samples')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an Optimizer\n",
    "So TensorFlow computes the gradients for you. But it gets even easier: it also provides a number of optimizers out of the box, including a Gradient Descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = 'y')\n",
    "theta  =  tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = 'theta')\n",
    "y_pred = tf.matmul(X, theta, name = 'predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = 'mse')\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum = 0.9)\n",
    "\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 200 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    y_pred_value = y_pred.eval()\n",
    "    best_theta = theta.eval()\n",
    "print(\"best_theta:\")   \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_sample = range(100);\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(housing.target[id_sample], 'b', label = 'housing.target')\n",
    "plt.plot(y_pred_value[id_sample], 'r', label = 'y_pred_values')\n",
    "plt.ylabel('House price')\n",
    "plt.xlabel('Samples')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding Data to the Training Algorithm\n",
    "\n",
    "Let’s try to modify the previous code to implement `Mini-batch Gradient Descent`. For this, we need a way to replace `X` and `y` at every iteration with the next mini-batch. The simplest way to do this is to use placeholder nodes. \n",
    "\n",
    "These nodes are special because they don’t actually perform any computation, they just output the data you tell them to output at runtime. They are typically used to pass the training data to TensorFlow during training. If you don’t specify a value at runtime for a placeholder, you get an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a placeholder node, you must call the `placeholder()` function and specify the output tensor’s data type. Optionally, you can also specify its shape, if you want to enforce it. If you specify None for a dimension, it means “any size.” For example, the following code creates a placeholder node A, and also a node B = A + 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.placeholder(tf.float32, shape = (None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict = {A: [[1,2,3]]})\n",
    "    B_val_2 = B.eval(feed_dict = {A: [[1,2,3], [11,12,13]]})\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we evaluate B, we pass a `feed_dict` to the `eval()` method that specifies the value of A. Note that A must have rank 2 (i.e., it must be two-dimensional) and there must be `three columns` (or else an exception is raised), but it can have any number of rows.\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    B_val_3 = B.eval(feed_dict = {A: [[1,2]]}) # this is wrong, feed must be three columns\n",
    "print(B_val_3)\n",
    "```\n",
    "<span style=\"color:red\"> ValueError: Cannot feed value of shape (1, 2) for Tensor 'Placeholder_1:0', which has shape '(?, 3)'</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Mini-batch Gradient Descent, we only need to tweak the existing code slightly. First change the definition of X and y in the construction phase to make them placeholder nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # reset the graph\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "batch_size = 5000\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name = 'X') # <-- change\n",
    "y = tf.placeholder(tf.float32, shape=(None,1), name = 'y')  # <-- change\n",
    "\n",
    "theta  =  tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = 'theta')\n",
    "y_pred = tf.matmul(X, theta, name = 'predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = 'mse')\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum = 0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = scaled_housing_data_plus_bias[indices] \n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval(feed_dict = {X:scaled_housing_data_plus_bias, y:housing.target.reshape(-1, 1)}))        \n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)    \n",
    "            sess.run(training_op, feed_dict = {X:X_batch, y:y_batch})            \n",
    "    y_pred_value = y_pred.eval(feed_dict = {X:scaled_housing_data_plus_bias, y:housing.target.reshape(-1, 1)})\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best_theta:\")   \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-END-**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
