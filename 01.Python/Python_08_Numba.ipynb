{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ~5 minute guide to Numba\n",
    "\n",
    "Numba is a just-in-time compiler for Python that works best on code that uses NumPy arrays and functions, and loops. The most common way to use Numba is through its collection of decorators that can be applied to your functions to instruct Numba to compile them. When a call is made to a Numba-decorated function it is compiled to machine code “just-in-time” for execution and all or part of your code can subsequently run at native machine code speed!\n",
    "\n",
    "Out of the box Numba works with the following:\n",
    "\n",
    "- OS: Windows (32 and 64 bit), OSX, Linux (32 and 64 bit). Unofficial support on *BSD.\n",
    "- Architecture: x86, x86_64, ppc64le, armv7l, armv8l (aarch64). Unofficial support on M1/Arm64.\n",
    "- GPUs: Nvidia CUDA.\n",
    "- CPython\n",
    "- NumPy 1.17 - latest\n",
    "\n",
    "Source: https://numba.readthedocs.io/en/stable/user/5minguide.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will Numba work for my code?\n",
    "\n",
    "This depends on what your code looks like, if your code is numerically orientated (does a lot of math), uses NumPy a lot and/or has a lot of loops, then Numba is often a good choice. In these examples we’ll apply the most fundamental of Numba’s JIT decorators, @jit, to try and speed up some functions to demonstrate what works well and what does not.\n",
    "\n",
    "Numba works well on code that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(100).reshape(10, 10)\n",
    "\n",
    "@jit(nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "def go_fast(a): # Function is compiled to machine code when called the first time\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "    return a + trace              # Numba likes NumPy broadcasting\n",
    "\n",
    "print(go_fast(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It won’t work very well, if at all, on code that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import pandas as pd\n",
    "\n",
    "x = {'a': [1, 2, 3], 'b': [20, 30, 40]}\n",
    "\n",
    "@jit\n",
    "def use_pandas(a): # Function will not benefit from Numba jit\n",
    "    df = pd.DataFrame.from_dict(a) # Numba doesn't know about pd.DataFrame\n",
    "    df += 1                        # Numba doesn't understand what this is\n",
    "    return df.cov()                # or this!\n",
    "\n",
    "print(use_pandas(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Pandas is not understood by Numba and as a result Numba would simply run this code via the interpreter but with the added cost of the Numba internal overheads!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to measure the performance of Numba?\n",
    "First, recall that Numba has to compile your function for the argument types given before it executes the machine code version of your function. This takes time. However, once the compilation has taken place Numba caches the machine code version of your function for the particular types of arguments presented. If it is called again with the same types, it can reuse the cached version instead of having to compile again.\n",
    "\n",
    "A really common mistake when measuring performance is to not account for the above behaviour and to time code once with a simple timer that includes the time taken to compile your function in the execution time.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "x = np.arange(100).reshape(10, 10)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def go_fast(a): # Function is compiled and runs in machine code\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):\n",
    "        trace += np.tanh(a[i, i])\n",
    "    return a + trace\n",
    "\n",
    "# DO NOT REPORT THIS... COMPILATION TIME IS INCLUDED IN THE EXECUTION TIME!\n",
    "start = time.time()\n",
    "go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (with compilation) = %s\" % (end - start))\n",
    "\n",
    "# NOW THE FUNCTION IS COMPILED, RE-TIME IT EXECUTING FROM CACHE\n",
    "start = time.time()\n",
    "go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (after compilation) = %s\" % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to measure the impact Numba JIT has on your code is to time execution using the timeit module functions; these measure multiple iterations of execution and, as a result, can be made to accommodate for the compilation time in the first execution."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67d32777f905cb443c7595cdac206bb322335f5bfbf8ae9544bce83f7472f717"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
